{
  "cells": [
    {
      "metadata": {
        "_uuid": "225708f447eee93041881f9d6c3a3e890cb16718"
      },
      "cell_type": "markdown",
      "source": "# Load Libraries for use in Binary Classification\nFirst let's import the module and create an environment."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from kaggle.competitions import twosigmanews\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nimport datetime as dt\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nstop = set(stopwords.words('english'))\n\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c20fa6deeac9d374c98774abd90bdc76b023ee63"
      },
      "cell_type": "code",
      "source": "(market_df, news_df) = env.get_training_data()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "59f41ff93251ae9f560fb6e0511c4ebbc81b2753"
      },
      "cell_type": "markdown",
      "source": "# Simple Data Preprocessing: Assume no feature is useless and remove any NaN"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1a72225ea18500d0a17a36250d6859c3f370d46a"
      },
      "cell_type": "code",
      "source": "#Lets combine some features in intuitive ways to reduce number of features\n#Market Data Reduction\nmarket_df['returnsOpenPrevRaw1_to_volume'] = market_df['returnsOpenPrevRaw1'] / market_df['volume'] #This normalizes the change in cost for one day to per stock [else larger companies could be unfairly weighted more]\nmarket_df['close_to_open'] = market_df['close'] / market_df['open'] #Really what matters is how much closing differs, not each indepedantly\nmarket_df['volume_to_mean'] = market_df['volume'] / market_df['volume'].mean() #This metric marks how much the company's total shares varies compared to the mean. If they're growing a lot or shrinking a lot, this will take that into account  \n\n#News Data Reduction\nnews_df['asset_sentiment_count'] = news_df.groupby(['assetName', 'sentimentClass'])['time'].transform('count') #How positively is the environment where an asset is mentioned\n\n#Merges Market Data and News Data into a single Training set\ndef data_prep(market_df,news_df):\n    market_df['time'] = market_df.time.dt.date\n    news_df['time'] = news_df.time.dt.hour\n    news_df['sourceTimestamp']= news_df.sourceTimestamp.dt.hour\n    news_df['firstCreated'] = news_df.firstCreated.dt.date\n    news_df['assetCodes'] = news_df['assetCodes'].map(lambda x: list(eval(x))[0])\n    news_df['headlineLen'] = news_df['headline'].apply(lambda x: len(x))\n    news_df['assetCodesLen'] = news_df['assetCodes'].apply(lambda x: len(x))\n    news_df['asset_sentence_mean'] = news_df.groupby(['assetName', 'sentenceCount'])['time'].transform('mean')\n    lbl = {k: v for v, k in enumerate(news_df['headlineTag'].unique())}\n    news_df['headlineTagT'] = news_df['headlineTag'].map(lbl)\n    kcol = ['firstCreated', 'assetCodes']\n    news_df = news_df.groupby(kcol, as_index=False).mean()\n\n    market_df = pd.merge(market_df, news_df, how='left', left_on=['time', 'assetCode'], \n                            right_on=['firstCreated', 'assetCodes'])\n\n    lbl = {k: v for v, k in enumerate(market_df['assetCode'].unique())}\n    market_df['assetCodeT'] = market_df['assetCode'].map(lbl)\n    \n    market_df = market_df.dropna(axis=0) #Drops any NaN errors\n    \n    return market_df\n\nmarket_train = data_prep(market_df, news_df)\nup = market_train.returnsOpenNextMktres10 >= 0 #Keep conditions where 10 day market price increased for our decision boundary\n\n#fcol = [c for c in market_train.columns if c not in ['assetCode', 'assetCodes', 'assetCodesLen', 'assetName', 'assetCodeT', 'volume_to_mean', 'sentence_word_count',\n                                            # 'firstCreated', 'headline', 'headlineTag', 'marketCommentary', 'provider', 'returnsOpenPrevRaw1_to_volume',\n                                            # 'returnsOpenNextMktres10', 'sourceId', 'subjects', 'time', 'time_x', 'universe','sourceTimestamp']]\n\nfcol = [c for c in market_train.columns if c not in['returnsOpenNextMktres10',\n                                                  'assetCode','assetName','assetCodesLen','assetCodes']] #Only drop strings, columns that are ALL NaN and 10 day market price, which we are predicting\n\nX = market_train[fcol].values\nup = up.values\nr = market_train.returnsOpenNextMktres10.values\n\n# Scaling of X values\nmins = np.min(X, axis=0)\nmaxs = np.max(X, axis=0)\nrng = maxs - mins\nX = 1 - ((maxs - X) / rng)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1a8bf276126f1b432ff704d8d0be865748971b0c"
      },
      "cell_type": "code",
      "source": "market_train.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cdae0cc5256bf9d93116d90718ec2785931e72b5"
      },
      "cell_type": "code",
      "source": "X_train, X_test, up_train, up_test, r_train, r_test = model_selection.train_test_split(X, up, r, test_size=0.2, random_state=99)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9fcf44900cca79786348324867da009bad9b96b0"
      },
      "cell_type": "code",
      "source": "params = {'learning_rate': 0.025, 'boosting': 'goss', 'objective': 'binary', 'metric': 'binary_logloss', 'is_training_metric': True, 'seed': 42}\nmodel = lgb.train(params, train_set=lgb.Dataset(X_train, label=up_train), num_boost_round=3000,\n                  valid_sets=[lgb.Dataset(X_train, label=up_train), lgb.Dataset(X_test, label=up_test)],\n                  verbose_eval=100, early_stopping_rounds=50)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "713b8cd68ab28b655a59a5408ffc3d00768a2d46"
      },
      "cell_type": "code",
      "source": "#Test Model\ny_pred = model.predict(X_test,num_iteration = model.best_iteration)\n#Determine Accuracy\ny_bool = y_pred >= 0 \nprint(y_bool)\nprint(up_test)\nacc = ~(y_bool ^ up_test) #XNOR operation so 1 1 and 0 0 returns true, else false\nprint(np.sum(acc)/len(acc))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d35554fb93152d26b4834ee30d3e16527871ad11"
      },
      "cell_type": "code",
      "source": "def generate_color():\n    color = '#{:02x}{:02x}{:02x}'.format(*map(lambda x: np.random.randint(0, 255), range(3)))\n    return color\n\ndf = pd.DataFrame({'imp': model.feature_importance(), 'col':fcol})\ndf = df.sort_values(['imp','col'], ascending=[True, False])\ndata = [df]\nfor dd in data:  \n    colors = []\n    for i in range(len(dd)):\n         colors.append(generate_color())\n\n    data = [\n        go.Bar(\n        orientation = 'h',\n        x=dd.imp,\n        y=dd.col,\n        name='Features',\n        textfont=dict(size=12),\n            marker=dict(\n            color= colors,\n            line=dict(\n                color='#000000',\n                width=0.5\n            ),\n            opacity = 0.87\n        )\n    )\n    ]\n    layout= go.Layout(\n        title= 'Feature Importance of LGB',\n        xaxis= dict(title='Columns', ticklen=5, zeroline=False, gridwidth=2),\n        showlegend=True\n    )\n\n    py.iplot(dict(data=data,layout=layout), filename='horizontal-bar')\n    print(df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9c55ed64ece1c4a393ab1cfd315f2463c5cd59ca"
      },
      "cell_type": "markdown",
      "source": "# How does the model change if we remove data during the crash and detect for clarical errors"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2a259846c74e58aab7ec40e08d1c2576810ca366"
      },
      "cell_type": "code",
      "source": "#Get fresh set of market and news data\n(market_train_df, news_train_df) = env.get_training_data()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6dbe5753b69863bd3200bf0f8de65b59fc06434a"
      },
      "cell_type": "code",
      "source": "#Recall from data analysis step, mean change gets crazy during financial crash and some entries are faulty [closing fluctuation is way too big for one day]\n\n#I noticed in a couple kernels that there are occassional errors in dataset. This occurs when an 'openning' price is greatly different then the average\nmarket_train_df['close_to_open'] =  np.abs(market_train_df['close'] / market_train_df['open']) #Normalized ratio of close price to open\n\nmarket_train_df['assetName_mean_open'] = market_train_df.groupby('assetName')['open'].transform('mean')\nmarket_train_df['assetName_mean_close'] = market_train_df.groupby('assetName')['close'].transform('mean')\nthreshold = 0.7\n\n# if open price is too far from mean open price for this company, replace it. Otherwise replace close price.\nfor i, row in market_train_df.loc[market_train_df['close_to_open'] >= (1+threshold)].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        market_train_df.iloc[i,5] = row['assetName_mean_open']\n    else:\n        market_train_df.iloc[i,4] = row['assetName_mean_close']\n        \nfor i, row in market_train_df.loc[market_train_df['close_to_open'] <= 1-(threshold/2)].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        market_train_df.iloc[i,5] = row['assetName_mean_open']\n    else:\n        market_train_df.iloc[i,4] = row['assetName_mean_close']\n        \n#Throw out data from financial crash\nmarket_train_df = market_train_df.loc[market_train_df['time'] >= '2010-01-01 22:00:00+0000']\nnews_train_df = news_train_df.loc[news_train_df['time'] >= '2010-01-01 22:00:00+0000']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c9b1fe54fcf2a2779a3a742a74b06a86164eeedc"
      },
      "cell_type": "code",
      "source": "#Merges Market Data and News Data into a single Training set\n#Market Data Reduction\nmarket_train_df['returnsOpenPrevRaw1_to_volume'] = market_train_df['returnsOpenPrevRaw1'] / market_train_df['volume'] #This normalizes the change in cost for one day to per stock [else larger companies could be unfairly weighted more]\nmarket_train_df['volume_to_mean'] = market_train_df['volume'] / market_train_df['volume'].mean() #This metric marks how much the company's total shares varies compared to the mean. If they're growing a lot or shrinking a lot, this will take that into account  \n\n#News Data Reduction\nnews_train_df['asset_sentiment_count'] = news_train_df.groupby(['assetName', 'sentimentClass'])['time'].transform('count') #How positively is the environment where an asset is mentioned\n\ndef data_prep(market_df,news_df):\n    market_df['time'] = market_df.time.dt.date\n    news_df['time'] = news_df.time.dt.hour\n    news_df['sourceTimestamp']= news_df.sourceTimestamp.dt.hour\n    news_df['firstCreated'] = news_df.firstCreated.dt.date\n    news_df['assetCodes'] = news_df['assetCodes'].map(lambda x: list(eval(x))[0])\n    news_df['headlineLen'] = news_df['headline'].apply(lambda x: len(x))\n    news_df['assetCodesLen'] = news_df['assetCodes'].apply(lambda x: len(x))\n    news_df['asset_sentence_mean'] = news_df.groupby(['assetName', 'sentenceCount'])['time'].transform('mean')\n    lbl = {k: v for v, k in enumerate(news_df['headlineTag'].unique())}\n    news_df['headlineTagT'] = news_df['headlineTag'].map(lbl)\n    kcol = ['firstCreated', 'assetCodes']\n    news_df = news_df.groupby(kcol, as_index=False).mean()\n\n    market_df = pd.merge(market_df, news_df, how='left', left_on=['time', 'assetCode'], \n                            right_on=['firstCreated', 'assetCodes'])\n\n    lbl = {k: v for v, k in enumerate(market_df['assetCode'].unique())}\n    market_df['assetCodeT'] = market_df['assetCode'].map(lbl)\n    \n    market_df = market_df.dropna(axis=0) #Drops any NaN errors\n    \n    return market_df\n\nmarket_train_2 = data_prep(market_train_df, news_train_df)\nup_2 = market_train_2.returnsOpenNextMktres10 >= 0 #Keep conditions where 10 day market price increased for our decision boundary\n\nfcol = [c for c in market_train_2.columns if c not in['returnsOpenNextMktres10',\n                                                  'assetCode','assetName','assetCodesLen','assetCodes']] #Only drop strings, columns that are ALL NaN and 10 day market price, which we are predicting\n\nX_2 = market_train_2[fcol].values\nup_2 = up_2.values\nr_2 = market_train_2.returnsOpenNextMktres10.values\n\n# Scaling of X values\nmins = np.min(X_2, axis=0)\nmaxs = np.max(X_2, axis=0)\nrng = maxs - mins\nX_2 = 1 - ((maxs - X_2) / rng)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ad34edcc4771ba1df8b2ded07acc754c7b1460a2"
      },
      "cell_type": "code",
      "source": "X_train_2, X_test_2, up_train_2, up_test_2, r_train_2, r_test_2 = model_selection.train_test_split(X_2, up_2, r_2, test_size=0.2, random_state=99)\n\nparams_2 = {'learning_rate': 0.025, 'boosting': 'gbdt', 'objective': 'binary', 'metric': 'binary_logloss', 'is_training_metric': True, 'seed': 42}\nmodel_2 = lgb.train(params_2, train_set=lgb.Dataset(X_train_2, label=up_train_2), num_boost_round=3000,\n                  valid_sets=[lgb.Dataset(X_train_2, label=up_train_2), lgb.Dataset(X_test_2, label=up_test_2)],\n                  verbose_eval=50, early_stopping_rounds=50)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "221e081decf61cfbbad43366377457d2f59bda68"
      },
      "cell_type": "code",
      "source": "#Test Model\ny_pred_2 = model_2.predict(X_test_2,num_iteration = model_2.best_iteration)\n#Determine Accuracy\ny_bool_2 = y_pred_2 >= 0 \nacc_2 = ~(y_bool_2 ^ up_test_2) #XNOR operation so 1 1 and 0 0 returns true, else false\nprint(np.sum(acc_2)/len(acc_2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "088bdf23d2de2f170cc084f4f01be014f3651c20"
      },
      "cell_type": "code",
      "source": "def generate_color():\n    color = '#{:02x}{:02x}{:02x}'.format(*map(lambda x: np.random.randint(0, 255), range(3)))\n    return color\n\ndf = pd.DataFrame({'imp': model_2.feature_importance(), 'col':fcol})\ndf = df.sort_values(['imp','col'], ascending=[True, False])\ndata = [df]\nfor dd in data:  \n    colors = []\n    for i in range(len(dd)):\n         colors.append(generate_color())\n\n    data = [\n        go.Bar(\n        orientation = 'h',\n        x=dd.imp,\n        y=dd.col,\n        name='Features',\n        textfont=dict(size=12),\n            marker=dict(\n            color= colors,\n            line=dict(\n                color='#000000',\n                width=0.5\n            ),\n            opacity = 0.87\n        )\n    )\n    ]\n    layout= go.Layout(\n        title= 'Feature Importance of LGB',\n        xaxis= dict(title='Columns', ticklen=5, zeroline=False, gridwidth=2),\n        showlegend=True\n    )\n\n    py.iplot(dict(data=data,layout=layout), filename='horizontal-bar')\n    print(df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1fa0fb4c5e8b02953ab8d11a55a1bff88b64ed61"
      },
      "cell_type": "markdown",
      "source": "# Model with lowest importance features removed"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9b0bec703676d7f431727795d52f151f190610a"
      },
      "cell_type": "code",
      "source": "#Get fresh set of market and news data\n(market_reduced_df, news_reduced_df) = env.get_training_data()\n\n#Lets combine some features in intuitive ways to reduce number of features\n#Market Data Reduction\nmarket_reduced_df['returnsOpenPrevRaw1_to_volume'] = market_reduced_df['returnsOpenPrevRaw1'] / market_reduced_df['volume'] #This normalizes the change in cost for one day to per stock [else larger companies could be unfairly weighted more]\nmarket_reduced_df['close_to_open'] = market_reduced_df['close'] / market_reduced_df['open'] #Really what matters is how much closing differs, not each indepedantly\nmarket_reduced_df['volume_to_mean'] = market_reduced_df['volume'] / market_reduced_df['volume'].mean() #This metric marks how much the company's total shares varies compared to the mean. If they're growing a lot or shrinking a lot, this will take that into account  \n\n#News Data Reduction\nnews_reduced_df['asset_sentiment_count'] = news_reduced_df.groupby(['assetName', 'sentimentClass'])['time'].transform('count') #How positively is the environment where an asset is mentioned\n\n#Merges Market Data and News Data into a single Training set\ndef data_prep(market_df,news_df):\n    market_df['time'] = market_df.time.dt.date\n    news_df['time'] = news_df.time.dt.hour\n    news_df['sourceTimestamp']= news_df.sourceTimestamp.dt.hour\n    news_df['firstCreated'] = news_df.firstCreated.dt.date\n    news_df['assetCodes'] = news_df['assetCodes'].map(lambda x: list(eval(x))[0])\n    news_df['headlineLen'] = news_df['headline'].apply(lambda x: len(x))\n    news_df['assetCodesLen'] = news_df['assetCodes'].apply(lambda x: len(x))\n    news_df['asset_sentence_mean'] = news_df.groupby(['assetName', 'sentenceCount'])['time'].transform('mean')\n    lbl = {k: v for v, k in enumerate(news_df['headlineTag'].unique())}\n    news_df['headlineTagT'] = news_df['headlineTag'].map(lbl)\n    kcol = ['firstCreated', 'assetCodes']\n    news_df = news_df.groupby(kcol, as_index=False).mean()\n\n    market_df = pd.merge(market_df, news_df, how='left', left_on=['time', 'assetCode'], \n                            right_on=['firstCreated', 'assetCodes'])\n\n    lbl = {k: v for v, k in enumerate(market_df['assetCode'].unique())}\n    market_df['assetCodeT'] = market_df['assetCode'].map(lbl)\n    \n    market_df = market_df.dropna(axis=0) #Drops any NaN errors\n    \n    return market_df\n\nmarket_train = data_prep(market_reduced_df, news_reduced_df)\nup = market_train.returnsOpenNextMktres10 >= 0 #Keep conditions where 10 day market price increased for our decision boundary\n\nfcol = [c for c in market_train.columns if c not in['returnsOpenNextMktres10',\n                                                  'assetCode','assetName','assetCodesLen','assetCodes',\n                                                   'volumn_to_mean','returnsOpenPrevRaw1_to_volume','firstCreated',\n                                                   'sourceTimestamp','universe','marketCommentary']] #Only drop strings, columns that are ALL NaN and 10 day market price, which we are predicting\n\nX_3 = market_train[fcol].values\nup_3 = up.values\nr_3 = market_train.returnsOpenNextMktres10.values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7410ee6fcb3272e45e11ef02baaca03b92601674"
      },
      "cell_type": "code",
      "source": "X_train_3, X_test_3, up_train_3, up_test_3, r_train_3, r_test_3 = model_selection.train_test_split(X_3, up_3, r_3, test_size=0.2, random_state=99)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "75633096082a7fbbbc71482fa272eb0ba876ded9"
      },
      "cell_type": "code",
      "source": "params_3 = {'learning_rate': 0.025, 'boosting': 'gbdt', 'objective': 'binary', 'metric': 'binary_logloss', 'is_training_metric': True, 'seed': 42}\nmodel_3 = lgb.train(params_3, train_set=lgb.Dataset(X_train_3, label=up_train_3), num_boost_round=3000,\n                  valid_sets=[lgb.Dataset(X_train_3, label=up_train_3), lgb.Dataset(X_test_3, label=up_test_3)],\n                  verbose_eval=100, early_stopping_rounds=50)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "746ded063807570fa6c232ccd7e8b3d915e3c681"
      },
      "cell_type": "code",
      "source": "#Test Model\ny_pred_3 = model_3.predict(X_test_3,num_iteration = model_3.best_iteration)\n#Determine Accuracy\ny_bool_3 = y_pred_3 >= 0 \nacc_3 = ~(y_bool_3 ^ up_test_3) #XNOR operation so 1 1 and 0 0 returns true, else false\nprint(np.sum(acc_3)/len(acc_3))",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}